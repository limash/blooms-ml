{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sgaHaBA4pFW0"
      },
      "source": [
        "## Check / Prepare data for algae blooms identification."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Kb2N9DLdKmQ"
      },
      "source": [
        "\n",
        "The idea is to take the modeled data and train a machine learning (ML) model on that data, then try to use on the observational data.\n",
        "The reason - models can't predict very well the exact time and location of algae blooms but they reproduce the physics/biogeochemistry of it.\n",
        "Thus, the intuition to check is that a ML model trained on modelled data will be able to predict blooms on observational data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VetmTphZWE-t"
      },
      "outputs": [],
      "source": [
        "import glob\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np  # noqa: F401\n",
        "import pandas as pd  # noqa: F401\n",
        "import xarray as xr\n",
        "import matplotlib.pyplot as plt  # noqa: F401\n",
        "\n",
        "from blooms_ml.utils import (\n",
        "    sample_stations,\n",
        "    extract_stations_rho,\n",
        "    extract_stations_u,\n",
        "    extract_stations_v,\n",
        "    merge_edges_to_centers,\n",
        "    append_rho_profiles,\n",
        "    normalize_series,\n",
        "    plot_variable,\n",
        "    plot_confusion_matrix,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ze_WckQlFgP"
      },
      "source": [
        "There is the output of hydrophysical+biogeochemical model of the Hardangerfjord at HPC FRAM.\n",
        "The files are very huge to download, so I have just mounted a data folder to use them.\n",
        "This is based on the ROMS hydrophysical and NERSEM biogeochemical models.\n",
        "Diagnostic files have data about PAR (photosynthetically active radiation).\n",
        "'Average' files have the rest of the variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "files_dia = sorted(glob.glob(f\"{Path.home()}/fram_shmiak/ROHO800_hindcast_2007_2019_v2bu/roho800_v2bu_dia/*dia*.nc\"))\n",
        "files_avg = sorted(glob.glob(f\"{Path.home()}/fram_shmiak/ROHO800_hindcast_2007_2019_v2bu/roho800_v2bu_avg/*avg*.nc\"))\n",
        "last_n_files = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8Bzl-FfV0en"
      },
      "outputs": [],
      "source": [
        "ds_dia = xr.open_mfdataset(files_dia[-last_n_files:])\n",
        "ds_avg = xr.open_mfdataset(files_avg[-last_n_files:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "stations, st_labels, xis, etas = sample_stations(ds_dia, 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "p = ds_dia.mask_rho.isel(ocean_time=-1).plot(\n",
        "    x=\"xi_rho\", y=\"eta_rho\", figsize=(14, 7), cmap='GnBu'\n",
        "    )\n",
        "p.axes.scatter(x=xis, y=etas, color='red')\n",
        "for i, label in enumerate(st_labels):\n",
        "    p.axes.annotate(label, (xis[i], etas[i]), color='red')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Extract light from the chosen points."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = extract_stations_rho(ds_dia, xis, etas)\n",
        "ds = merge_edges_to_centers(ds)\n",
        "df_dia = ds[['light_PAR0', 'P1_netPI']].to_dataframe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPWnLmJEiJ6W"
      },
      "source": [
        "Extract other variables.\n",
        "There are too many variables, let's take only some of them."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EZed2-5XZdJ6"
      },
      "outputs": [],
      "source": [
        "vars = ['lat_rho', 'lon_rho', 'ocean_time', 's_rho',\n",
        "        'TotChl', 'P1_c',\n",
        "        'swradWm2',\n",
        "        'rho', 'temp', 'salt', 'AKv', 'u', 'v', 'w',\n",
        "        'N1_p', 'N3_n', 'N5_s', 'O2_o']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_rho = extract_stations_rho(ds_avg, xis, etas)\n",
        "ds_rho = ds_rho.drop_dims(['eta_u', 'eta_v', 'eta_psi', 'xi_u', 'xi_v', 'xi_psi' ])\n",
        "ds_u = extract_stations_u(ds_avg, xis, etas)\n",
        "ds_u = ds_u.drop_dims(['eta_rho', 'eta_v', 'eta_psi', 'xi_rho', 'xi_v', 'xi_psi' ])\n",
        "ds_v = extract_stations_v(ds_avg, xis, etas)\n",
        "ds_v = ds_v.drop_dims(['eta_rho', 'eta_u', 'eta_psi', 'xi_rho', 'xi_u', 'xi_psi' ])\n",
        "ds = xr.merge([ds_rho, ds_u, ds_v])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = merge_edges_to_centers(ds)\n",
        "ds_subset = ds.drop_vars([var for var in ds.variables if var not in vars])\n",
        "df = ds_subset.to_dataframe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df['light_PAR0'] = df_dia['light_PAR0']\n",
        "df['P1_netPI'] = df_dia['P1_netPI']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_station = df.loc[df.index.get_level_values('station') == 3]\n",
        "df_station = df_station.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_station"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='light_PAR0').iloc[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sVU7nKPvI1_p"
      },
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='P1_c').iloc[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='P1_netPI').iloc[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='w').iloc[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJ8g49u8doWi"
      },
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='rho').iloc[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vjuKnb-rGH_j"
      },
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='N1_p').iloc[::-1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0GTxQkdGI9A"
      },
      "outputs": [],
      "source": [
        "plot_variable(df_station.pivot(index='s_rho', columns='ocean_time', values='N3_n').iloc[::-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data preprocessing.\n",
        "Using equation of state it is possible to recover density form temperature and salinity.\n",
        "Extract and append rho profiles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input = df.drop(columns=['lon_rho', 'lat_rho', 'temp', 'salt', 'u', 'v', 'O2_o', 'AKv'])\n",
        "df_input = append_rho_profiles(df_input)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# no light, no blooms, remove dark points, it will help with scaling and sampling\n",
        "df_input = df_input[df_input['light_PAR0'] > 10]\n",
        "df_input = df_input.reset_index(drop=True)\n",
        "df_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input['label'] = (df_input['P1_netPI'] > 18).astype(np.float32)\n",
        "df_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input = df_input.drop(columns=['ocean_time', 's_rho'])\n",
        "df_input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input.iloc[:, :8] = df_input.iloc[:, :8].apply(normalize_series, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input['light_PAR0'].plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "(df_input['label'] == 1).sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_input"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Features correlation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df = df_input.iloc[:, [0, 1, 2, 3, 4, 5, 6, 7, -1]].sample(frac=1)  # exclude rho profiles\n",
        "df_bloom = df.loc[df['label'] == 1]\n",
        "df_clean = df.loc[df['label'] == 0][:(df_input['label'] == 1).sum()]\n",
        "normal_distributed_df = pd.concat([df_bloom, df_clean])\n",
        "new_df = normal_distributed_df.sample(frac=1, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10))\n",
        "\n",
        "corr = df.corr()\n",
        "sns.heatmap(corr, cmap='coolwarm_r', annot_kws={'size':10}, ax=ax1)\n",
        "ax1.set_title(\"Imbalanced Correlation Matrix\", fontsize=14)\n",
        "\n",
        "sub_sample_corr = new_df.corr()\n",
        "sns.heatmap(sub_sample_corr, cmap='coolwarm_r', annot_kws={'size':10}, ax=ax2)\n",
        "ax2.set_title(\"SubSample Correlation Matrix\", fontsize=14)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unsupervised"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from sklearn.manifold import TSNE\n",
        "from sklearn.decomposition import PCA, TruncatedSVD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df = new_df.drop(columns=[\"P1_netPI\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "new_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# New_df is from the random undersample data (fewer instances)\n",
        "X = new_df.drop('label', axis=1)\n",
        "y = new_df['label']\n",
        "\n",
        "# T-SNE Implementation\n",
        "t0 = time.time()\n",
        "X_reduced_tsne = TSNE(n_components=2, random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(f\"T-SNE took {t1 - t0:.2} s\")\n",
        "\n",
        "# PCA Implementation\n",
        "t0 = time.time()\n",
        "X_reduced_pca = PCA(n_components=2, random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(f\"PCA took {t1 - t0:.2} s\")\n",
        "\n",
        "# TruncatedSVD\n",
        "t0 = time.time()\n",
        "X_reduced_svd = TruncatedSVD(n_components=2, algorithm='randomized', random_state=42).fit_transform(X.values)\n",
        "t1 = time.time()\n",
        "print(f\"Truncated SVD took {t1 - t0:.2} s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.patches as mpatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "f, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(24,6))\n",
        "f.suptitle('Clusters using Dimensionality Reduction', fontsize=14)\n",
        "\n",
        "blue_patch = mpatches.Patch(color='#0A0AFF', label='No bloom')\n",
        "red_patch = mpatches.Patch(color='#AF0000', label='bloom')\n",
        "\n",
        "# t-SNE scatter plot\n",
        "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 0), cmap='coolwarm', label='No bloom', linewidths=2)\n",
        "ax1.scatter(X_reduced_tsne[:,0], X_reduced_tsne[:,1], c=(y == 1), cmap='coolwarm', label='bloom', linewidths=2)\n",
        "ax1.set_title('t-SNE', fontsize=14)\n",
        "\n",
        "ax1.grid(True)\n",
        "\n",
        "ax1.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "# PCA scatter plot\n",
        "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 0), cmap='coolwarm', label='No bloom', linewidths=2)\n",
        "ax2.scatter(X_reduced_pca[:,0], X_reduced_pca[:,1], c=(y == 1), cmap='coolwarm', label='bloom', linewidths=2)\n",
        "ax2.set_title('PCA', fontsize=14)\n",
        "\n",
        "ax2.grid(True)\n",
        "\n",
        "ax2.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "# TruncatedSVD scatter plot\n",
        "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 0), cmap='coolwarm', label='No bloom', linewidths=2)\n",
        "ax3.scatter(X_reduced_svd[:,0], X_reduced_svd[:,1], c=(y == 1), cmap='coolwarm', label='bloom', linewidths=2)\n",
        "ax3.set_title('Truncated SVD', fontsize=14)\n",
        "\n",
        "ax3.grid(True)\n",
        "\n",
        "ax3.legend(handles=[blue_patch, red_patch])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "from sklearn.metrics import confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = df_input.drop(columns=['P1_netPI', 'label'])\n",
        "y = df_input['label']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "classifiers = {\n",
        "    \"LogisiticRegression\": LogisticRegression(),\n",
        "    \"KNearest\": KNeighborsClassifier(),\n",
        "    \"Support Vector Classifier\": SVC(),\n",
        "    \"DecisionTreeClassifier\": DecisionTreeClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "for key, classifier in classifiers.items():\n",
        "    classifier.fit(X_train, y_train)\n",
        "    training_score = cross_val_score(classifier, X_test, y_test, cv=5)\n",
        "    print(\"Classifiers: \", classifier.__class__.__name__, \"Has a training score of\", round(training_score.mean(), 2) * 100, \"% accuracy score\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_reg_sm = LogisticRegression()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "log_reg_sm.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_pred_log_reg = log_reg_sm.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "labels = ['No bloom', 'bloom']\n",
        "plot_confusion_matrix(confusion_matrix(y_test, y_pred_log_reg), labels, title=\"Confusion Matrix\", cmap=plt.cm.Reds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
